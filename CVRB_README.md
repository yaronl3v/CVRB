# Code-Verified Reasoning Benchmark (CVRB)

CVRB tests how well large language models can reason and discover patterns in a two-phase *self-play* loop: first, Creator models invent miniature deterministic *worlds*; then Solver models try to answer questions about those worlds.  All answers are verified automatically by running the code, not by humans.

---
## 1  What makes CVRB different?

1. **Self-play benchmarking.**  The very models we want to evaluate first *create* brand-new worlds and then *solve* them, ensuring the test set is always novel and shaped by current capabilities.
2. **Evergreen difficulty.**  Because worlds are machine-generated, their complexity can be tuned through the creator prompt or by simply upgrading the Creator model.  No human curation is needed and any Solver model can plug in at any time.
3. **Code-verified answers.**  Every solution is checked by running the simulation itself, so results are objective and reproducible.

---

>  This file intentionally contains *no executable code* so it can serve as a theory-only overview.  For implementation details, CLI commands and developer setup please refer to the standard `README.md`.

## 2  How it works

CVRB defines three autonomous roles:

| Role      | Responsibility                                                     |
|-----------|--------------------------------------------------------------------|
| **Creator**   | Generates a brand-new world and a set of questions about it.
| **Validator** | Confirms the world is internally consistent, deterministic and fully solvable.
| **Solver**    | Analyses the world and returns a JSON object containing answers plus a short justification.

The evaluation pipeline is fully automated:

1. A Creator model proposes one or more candidate worlds.
2. Independent Validators run each world to make sure the rules are met and every question has a unique answer.
3. Solver models receive only the natural-language description of the world.  They write code that reasons about the simulation and outputs their answers.
4. The framework executes that code; outputs are compared against ground-truth generated by running the simulation directly.

---
## 3  Role workflows

### Creator

1. **Input** – a high-level prompt that defines the game rules, the JSON schema for questions and acceptable code style.
2. **Output** –
   * A natural-language description of the world and its questions.
   * Reference simulation code that produces ground-truth answers.
3. The Creator stores the new world in the database with a temporary status of *unvalidated* and assigns it to a **set id** (default `0`).

### Validator

1. **Input** – the world description and the expected answers produced by the Creator’s reference code (but *not* the code itself).
2. The Validator writes its *own* simulation from scratch and runs it.
3. If the outputs match the Creator’s answers bit-for-bit and the run is deterministic (no randomness, no external I/O), the world is marked **validated**; otherwise it is rejected.

### Solver

1. **Input** – only the natural-language world description and its questions; no reference code or answers.
2. The Solver generates code or declarative logic that reasons about the simulation and outputs a JSON answer object.
3. The framework executes this code and compares the answers against the ground truth.  Each solver attempt is recorded in the `solutions` table with accuracy and run-time metrics.

---
## 4  Scoring

CVRB now derives a **quality score** for every world based on how widely it separates solver performance.  The calculation, implemented in `server/src/CVRB/helpers/quality.js`, is:

```
quality = (0.4 × spread + 0.4 × pair_avg + 0.2 × distinct) / Q
```

where

* **spread** – max(score) − min(score)
* **pair_avg** – mean absolute gap between every pair of solver scores
* **distinct** – number of distinct score levels minus 1
* **Q** – theoretical maximum (5 for 0-5 ratings, 100 for percentages)

The raw result is clamped to the 0-1 range; higher values indicate a world that better discriminates between solvers.

We still log per-solver metrics such as **accuracy**, **failure rate** and run-time, but the *quality score* is the principal figure used to rank and filter worlds.

---
## 5  Selecting & promoting worlds

CVRB continuously generates candidate worlds, but only the most *informative* ones are promoted to the public benchmark (**set&nbsp;1**). The promotion logic implemented in `server/src/CVRB/helpers/world_promotion.js` works as follows:

1. **Rank by quality score.**  All validated worlds are sorted by the quality score described above.
2. **Promote the top _N_.**  The highest-scoring N worlds (default **N&nbsp;= 2**) are promoted automatically.
3. **Exceptional worlds.**  Any additional worlds whose quality score exceeds an *exceptional* threshold (default **0.4**) are also promoted.
4. **Update set id.**  Promoted worlds have their `set_id` changed to **1**, making them visible in the front-end by default.

This automated pipeline keeps the benchmark fresh while guaranteeing every published world meaningfully separates solver performance.

---
## 6  Project layout (high-level)

* **`server/`** – Node .js backend that houses the Creator, Validator and Solver pipelines and stores everything in PostgreSQL (with the PostGIS extension for spatial worlds).
* **`client/`** – React application that lets humans browse worlds, solver attempts and scores.



---
## 7  Front-end explorer (React client)

The React interface found in the `client/` folder lets researchers:

* Browse a list of all validated worlds.
* Inspect the natural-language description and underlying simulation code.
* View every solver attempt with its answer JSON, justification snippet and score.
* Compare models side-by-side using sortable tables and charts.

Each world is assigned a **set id**, which defaults to `0`. Users can change a world's set id through the front-end to group or select worlds as needed. The UI allows filtering and viewing worlds by set id, making it easy to focus on particular sets or batches. 

---
## 8  Why you might use CVRB

* Benchmark new LLM releases for algorithmic reasoning.
* Stress-test tool-use abilities – models must combine language understanding with code generation.
* Research curriculum learning: gradually increase world complexity to train more capable agents.
* Study emergent behaviour as Creator models iterate and bootstrap increasingly complex worlds.

---
_Last updated: <!-- date will be filled automatically by project tooling -->_
